{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jvlia17/blab/blob/main/Emotion_analysis_with_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDxcljnEbMXK"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WmYkd0OmbOWc"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, Embedding, Input, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 2000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecgDQ7IObqGA"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klD8dE3qbrll",
        "outputId": "959a635a-ccc7-4e30-f728-6e0efcc319aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-29 10:01:03--  https://www.dropbox.com/s/845gac1uofd2mmz/NLP.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/845gac1uofd2mmz/NLP.zip [following]\n",
            "--2023-05-29 10:01:04--  https://www.dropbox.com/s/raw/845gac1uofd2mmz/NLP.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com/cd/0/inline/B88AN57DtGfttW7AIW2FID2eFBXS-7wjy-rFYG3-DWHMdido5szv5TCaUzsuskR2EAVjGZU3vHNO8B5HTaMer0Ui9Oz5M-sNnRvS5He3pk6LHQTJdioUuRVIITlP1BCl4d3bA0sL3d_I11y5ctIJBulKs_clsGloy6rYeW25KJLUFg/file# [following]\n",
            "--2023-05-29 10:01:04--  https://uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com/cd/0/inline/B88AN57DtGfttW7AIW2FID2eFBXS-7wjy-rFYG3-DWHMdido5szv5TCaUzsuskR2EAVjGZU3vHNO8B5HTaMer0Ui9Oz5M-sNnRvS5He3pk6LHQTJdioUuRVIITlP1BCl4d3bA0sL3d_I11y5ctIJBulKs_clsGloy6rYeW25KJLUFg/file\n",
            "Resolving uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com (uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com (uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B88zAQ6Ox9t-kkoJvJoFLXhS0CvIaaNtSTqCoyBVp-bbkbtIAcLLqhiSKKBz5p0TeKF9VI-LA7N-i1zhlOW8fv-FZVRqllNDwXudoRE-Ow7DhTX_hxR2jtT07YizcUjnpIm79OIXp3MO0Fv7Wj4l3HsFLekpshA02VSwADeo1D7QLkHHTcxiGhSv2uw3TMSulBkQ0BIHJHsVxjd3viqGR2EKTY952IvtI0LkvY4C7FnZIwXnFOxxi5l6GAk6wEAxHkQk3l3_RVRekdDt2-q0WDklhrmKIMnKmKBt-dET2-3lPKgpQljsiE3vbVF1kknBCjVDRIM0SdZ4bZ4cnGW4j5mWk336Q86kNq8bgltGKA2gUHsPsh8WRiAgQbH64wVUhmy5nnatR4JSqh6rkMC1xawhwOmc4pxDOq7q0ccfly1pAg/file [following]\n",
            "--2023-05-29 10:01:05--  https://uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com/cd/0/inline2/B88zAQ6Ox9t-kkoJvJoFLXhS0CvIaaNtSTqCoyBVp-bbkbtIAcLLqhiSKKBz5p0TeKF9VI-LA7N-i1zhlOW8fv-FZVRqllNDwXudoRE-Ow7DhTX_hxR2jtT07YizcUjnpIm79OIXp3MO0Fv7Wj4l3HsFLekpshA02VSwADeo1D7QLkHHTcxiGhSv2uw3TMSulBkQ0BIHJHsVxjd3viqGR2EKTY952IvtI0LkvY4C7FnZIwXnFOxxi5l6GAk6wEAxHkQk3l3_RVRekdDt2-q0WDklhrmKIMnKmKBt-dET2-3lPKgpQljsiE3vbVF1kknBCjVDRIM0SdZ4bZ4cnGW4j5mWk336Q86kNq8bgltGKA2gUHsPsh8WRiAgQbH64wVUhmy5nnatR4JSqh6rkMC1xawhwOmc4pxDOq7q0ccfly1pAg/file\n",
            "Reusing existing connection to uc2f19af16ebf2cade6e27f1ca38.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2243185 (2.1M) [application/zip]\n",
            "Saving to: ‘NLP.zip?dl=0’\n",
            "\n",
            "NLP.zip?dl=0        100%[===================>]   2.14M  1.88MB/s    in 1.1s    \n",
            "\n",
            "2023-05-29 10:01:07 (1.88 MB/s) - ‘NLP.zip?dl=0’ saved [2243185/2243185]\n",
            "\n",
            "Archive:  /content/NLP.zip?dl=0\n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "# Delete existing folders\n",
        "!rm -rf sample_data\n",
        "\n",
        "# Get the data and unzip it\n",
        "!wget https://www.dropbox.com/s/845gac1uofd2mmz/NLP.zip?dl=0\n",
        "!unzip /content/NLP.zip?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frDxH5s5bPQj"
      },
      "source": [
        "Reading Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FLEVwsIObSW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc327005-e8ce-47d3-fbf6-95da3fe3852c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text            35546\n",
              "fear                2\n",
              "disgust             2\n",
              "anger               2\n",
              "joy                 2\n",
              "surprise            2\n",
              "trust               2\n",
              "sadness             2\n",
              "anticipation        2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Reading input csv files\n",
        "train = pd.read_csv(\"train.csv\", on_bad_lines=\"skip\")\n",
        "test = pd.read_csv(\"test.csv\", on_bad_lines=\"skip\")\n",
        "train.head(10)\n",
        "train.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKEJAFnebXrh",
        "outputId": "f58665df-da38-4e6e-87b7-2b4e57f4816a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First comment text in training set:\n",
            "\n",
            " @kole_holland don't fret  there's another game on at 8 #staytuned\n"
          ]
        }
      ],
      "source": [
        "# Do not change\n",
        "preprocess = False\n",
        "#Assinging the texts to list of strings\n",
        "train_texts = train.text.values\n",
        "test_texts = test.text.values\n",
        "#Assignings the labels as a separate df\n",
        "labels = ['fear','disgust','anger','joy','surprise','trust','sadness','anticipation']\n",
        "train_labels = train[labels]\n",
        "test_labels = test[labels]\n",
        "#Printing the list of strings\n",
        "print(\"First comment text in training set:\\n\\n\", test_texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIs63zyEP8iI"
      },
      "source": [
        "Optional - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE-YbsyIP-VQ",
        "outputId": "cdc9f3fc-bf82-4548-f04a-c290ba8e9f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Does it make sense to make preprocessor a class, since it will mosty likely have only constant attributes, which could be just global variables?\n",
        "# Probably not, although if at some point necessity for an internal variable for instance appears, class objects will allow it and global variables would not\n",
        "class Preprocessor:\n",
        "    def __init__(self):\n",
        "        self.__token_url = \"URL\"\n",
        "        self.__token_user = \"USER\"\n",
        "    def __strip_urls(self, text):\n",
        "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "        urls = re.findall(regex,text)\n",
        "        for url in urls:\n",
        "            text = text.replace(url[0], self.__token_url)\n",
        "        return text\n",
        "    def __strip_users(self, text):\n",
        "        for word in text.split():\n",
        "            if(word[0] == \"@\"):\n",
        "                text = text.replace(word, self.__token_user)\n",
        "        return text\n",
        "    def __strip_stopwords(self, text):\n",
        "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "        words = []\n",
        "        for word in self.WordTokenize(text):\n",
        "            if word not in stopwords:\n",
        "                words.append(word)\n",
        "        return ' '.join(words)\n",
        "    def __lemmatize(self, text):\n",
        "        words = []\n",
        "        lemmatize = nltk.WordNetLemmatizer().lemmatize\n",
        "        for word in self.WordTokenize(text):\n",
        "            words.append( lemmatize(word) )\n",
        "        return ' '.join(words)\n",
        "    def __strip_punctuation(self, text):\n",
        "        punctuation = string.punctuation\n",
        "        return text.translate(str.maketrans('', '', punctuation))\n",
        "    def WordTokenize(self, text):\n",
        "        return word_tokenize(text)\n",
        "    def Preprocess(self, text):\n",
        "        text = text.lower()\n",
        "        text = self.__strip_urls(text)\n",
        "        text = self.__strip_users(text)\n",
        "        text = self.__lemmatize(text)\n",
        "        text = self.__strip_punctuation(text)\n",
        "        text = self.__strip_stopwords(text)\n",
        "        return text\n",
        "\n",
        "# Preprocessing itself\n",
        "p = Preprocessor()\n",
        "preprocess = True\n",
        "train_texts = [ p.Preprocess(i) for i in train_texts ]\n",
        "test_texts = [ p.Preprocess(i) for i in test_texts ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW0h3-pBbbey"
      },
      "source": [
        "Vectorizers - use only one, depending which task you're solving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0SE5UmbvUnwk"
      },
      "outputs": [],
      "source": [
        "# For embedding (word2vec, glove, fasttext)\n",
        "class EmbedVectorizer:\n",
        "  def __init__(self):\n",
        "    self.__v = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "    self.DO_NOT_USE = self.__v\n",
        "  def fit(self, texts):\n",
        "    self.__v.fit_on_texts(texts)\n",
        "  def convert(self, texts):\n",
        "    return pad_sequences(self.__v.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n",
        "vectorizer = EmbedVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd52aerrWKL-"
      },
      "outputs": [],
      "source": [
        "# For Bag of Words\n",
        "class BowVectorizer:\n",
        "  def __init__(self):\n",
        "    self.__v = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "  def fit(self, texts):\n",
        "    self.__v.fit_on_texts(texts)\n",
        "  def convert(self, texts):\n",
        "    return self.__v.texts_to_matrix(texts, mode = 'count')\n",
        "vectorizer = BowVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZjpEbnweWY3"
      },
      "outputs": [],
      "source": [
        "# For TF-IDF\n",
        "class TfIdfVectorizer:\n",
        "  def __init__(self):\n",
        "    self.__v = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "  def fit(self, texts):\n",
        "    self.__v.fit_on_texts(texts)\n",
        "  def convert(self, texts):\n",
        "    return self.__v.texts_to_matrix(texts, mode = 'tfidf')\n",
        "vectorizer = TfIdfVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1nLyb3_Pv9K"
      },
      "source": [
        "Use vectorizer to convert strings into feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nIbHsVNIbb86"
      },
      "outputs": [],
      "source": [
        "#Updates internal vocabulary based on a list of texts.\n",
        "vectorizer.fit(train_texts)\n",
        "#Transforms each text in texts to a sequence of integers.\n",
        "train_sequences = vectorizer.convert(train_texts)\n",
        "test_sequences = vectorizer.convert(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxB9abOmOwd6",
        "outputId": "83639d14-b751-4a25-93a6-4accb0c4d9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    7    7\n",
            "    7 1204  360   33   59 1521]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    4   10    3    4   10   42  602  685  423 1737   57   61\n",
            "  486  224  747   42  123  126]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  522   12  195  208    5   38  170   18  686  230  472  547 1407  547\n",
            "   22  547   22   41 1813   15]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0 1174  531 1075]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    3   10    8    9  143  135   88  145 1062  358  193   61   95   44\n",
            "   88   10 1854   15  706  127]\n"
          ]
        }
      ],
      "source": [
        "#Display vectorized data (just for debugging) \n",
        "for i in train_sequences[0:5]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CL02NZdblVn"
      },
      "source": [
        "**OSADZANIE SŁÓW: Defining a 1D CNN model for Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model dla customowego (autorskiego) osadzania słów"
      ],
      "metadata": {
        "id": "pgtT3VkFImF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM))"
      ],
      "metadata": {
        "id": "bKKtfRx9IVYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podejście Glove"
      ],
      "metadata": {
        "id": "mDGtXSB5J7Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to download glove data\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94hJWcy2rf9C",
        "outputId": "437ca8b1-0c93-422a-8114-4c23e734002e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-29 10:02:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-05-29 10:02:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-29 10:02:28--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.98MB/s    in 2m 41s  \n",
            "\n",
            "2023-05-29 10:05:10 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100 \n",
        "\n",
        "embeddings_index = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:],dtype = 'float32') #store all excepy val[0]\n",
        "    embeddings_index[word] = coefs #populate embedding dict\n",
        "    \n",
        "f.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors') \n",
        "\n",
        "# prepare embedding_matrix\n",
        "embedding_matrix = np.zeros((MAX_NUM_WORDS,EMBEDDING_DIM))\n",
        "\n",
        "for word,index in vectorizer.DO_NOT_USE.word_index.items():\n",
        "    \n",
        "    if index> MAX_NUM_WORDS -1: #bounding by max feat\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADowMxLFJkjC",
        "outputId": "d4fe571f-90b1-4f14-8396-3d2dbb4a8c03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model dla Glove"
      ],
      "metadata": {
        "id": "yDgOE-eXUkEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mNs4N3tIblva"
      },
      "outputs": [],
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, trainable = False, embeddings_initializer = Constant(embedding_matrix)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podejście FastText"
      ],
      "metadata": {
        "id": "xFQq80hnSskv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to download data\n",
        "!wget https://www.dropbox.com/s/q419rkb3zomh1gx/wiki.simple.vec.zip?dl=0\n",
        "!unzip -q /content/wiki.simple.vec.zip?dl=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyn-Ee4ntbnp",
        "outputId": "16252139-5ff6-4271-90ba-e407fcf726a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-28 19:23:13--  https://www.dropbox.com/s/q419rkb3zomh1gx/wiki.simple.vec.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/q419rkb3zomh1gx/wiki.simple.vec.zip [following]\n",
            "--2023-05-28 19:23:13--  https://www.dropbox.com/s/raw/q419rkb3zomh1gx/wiki.simple.vec.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc680a51866136912203c832ad34.dl.dropboxusercontent.com/cd/0/inline/B87Lx3e2reVqD4e3IBRbVoIW-0uwHIrVneRZNGZwn-NpL9eQOgBhemgfi9JmXBNCSMIiJUdhH4zP96hzeYOtSnx2FBtiaCoYDzlgx3e1vts2HG4dqs8afvY-L3XHJGq7C5qscQgcTCJPGR78zuQV65EWdSe_yX8BcKS4BqJ6onafog/file# [following]\n",
            "--2023-05-28 19:23:13--  https://uc680a51866136912203c832ad34.dl.dropboxusercontent.com/cd/0/inline/B87Lx3e2reVqD4e3IBRbVoIW-0uwHIrVneRZNGZwn-NpL9eQOgBhemgfi9JmXBNCSMIiJUdhH4zP96hzeYOtSnx2FBtiaCoYDzlgx3e1vts2HG4dqs8afvY-L3XHJGq7C5qscQgcTCJPGR78zuQV65EWdSe_yX8BcKS4BqJ6onafog/file\n",
            "Resolving uc680a51866136912203c832ad34.dl.dropboxusercontent.com (uc680a51866136912203c832ad34.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc680a51866136912203c832ad34.dl.dropboxusercontent.com (uc680a51866136912203c832ad34.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B87VM1HEI0_rfiT-Sm--UHenl5tq3uVt-PPwJY3s943dKTn8aaU8RFMdOuXX5sNPoog6YYHvqyfdjlaPswFBaF82MptGoAXTP7Hyn2zJn8z3CiRvMlV4O2oFx1zFQ3Gy4wMFM85xCavSqK1s_wE4H5hFqwP4NSalYCL8V6NpRqvHIctHBv5qK2Nfq6vAIED98p4F7f4RGcW9RSscQDj4N7IrHD8ZP1qQTmosc0iy-jFEiOFSglExVB7uEmlxaPeqwjN9mTWC1Ak2r9K2jiCCimYrISCaZZUhTWDaERwbERhgM8MixKrshLjtCL7d23F_uir09wqvgRSDRz5qqna8KfCmYRK-5qtbGKW-OrhTpoe6IrIti--8PeD_gGGeg1aXxhqrEXLqMtZteXaJh7s79XaThg2JKQOfXRtUWz7hhbDkrA/file [following]\n",
            "--2023-05-28 19:23:14--  https://uc680a51866136912203c832ad34.dl.dropboxusercontent.com/cd/0/inline2/B87VM1HEI0_rfiT-Sm--UHenl5tq3uVt-PPwJY3s943dKTn8aaU8RFMdOuXX5sNPoog6YYHvqyfdjlaPswFBaF82MptGoAXTP7Hyn2zJn8z3CiRvMlV4O2oFx1zFQ3Gy4wMFM85xCavSqK1s_wE4H5hFqwP4NSalYCL8V6NpRqvHIctHBv5qK2Nfq6vAIED98p4F7f4RGcW9RSscQDj4N7IrHD8ZP1qQTmosc0iy-jFEiOFSglExVB7uEmlxaPeqwjN9mTWC1Ak2r9K2jiCCimYrISCaZZUhTWDaERwbERhgM8MixKrshLjtCL7d23F_uir09wqvgRSDRz5qqna8KfCmYRK-5qtbGKW-OrhTpoe6IrIti--8PeD_gGGeg1aXxhqrEXLqMtZteXaJh7s79XaThg2JKQOfXRtUWz7hhbDkrA/file\n",
            "Reusing existing connection to uc680a51866136912203c832ad34.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111680439 (107M) [application/zip]\n",
            "Saving to: ‘wiki.simple.vec.zip?dl=0’\n",
            "\n",
            "wiki.simple.vec.zip 100%[===================>] 106.51M  74.5MB/s    in 1.4s    \n",
            "\n",
            "2023-05-28 19:23:16 (74.5 MB/s) - ‘wiki.simple.vec.zip?dl=0’ saved [111680439/111680439]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, csv, math, codecs\n",
        "from tqdm import tqdm\n",
        "\n",
        "#load embeddings\n",
        "embeddings_index = {}\n",
        "f = codecs.open('/content/wiki.simple.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utqJ5oagSuMu",
        "outputId": "4b44aaeb-2fa9-4f01-d42f-2f1cf846ee01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "111052it [00:10, 10446.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 111052 word vectors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 300\n",
        "\n",
        "#embedding matrix\n",
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NUM_WORDS, len(vectorizer.DO_NOT_USE.word_index))\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in vectorizer.DO_NOT_USE.word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAyHS49qb7pn",
        "outputId": "6bda7262-6d4b-4d29-ad1a-81d0e1e4bfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model dla FastText"
      ],
      "metadata": {
        "id": "tbVpCBbPb5hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(nb_words, EMBEDDING_DIM,\n",
        "          weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))"
      ],
      "metadata": {
        "id": "f7vQQRt7bzc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podejście Word2Vec"
      ],
      "metadata": {
        "id": "RIGArOSigz0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "vocab_size = len(vectorizer.DO_NOT_USE.word_index) + 1\n",
        "\n",
        "word2vec_model = Word2Vec([w.split() for w in train_texts], vector_size = EMBEDDING_DIM, window=3, min_count=1, workers=16)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in vectorizer.DO_NOT_USE.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n"
      ],
      "metadata": {
        "id": "5FLCCHnUkNKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model dla Word2Vec"
      ],
      "metadata": {
        "id": "tePGRjSck6Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))"
      ],
      "metadata": {
        "id": "ad-y9pZyk7wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADAcbwwGTsXb"
      },
      "source": [
        "CZĘSTOŚĆ SŁÓW: CNN  model for BoW / similar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Input(shape = (MAX_NUM_WORDS,1)))"
      ],
      "metadata": {
        "id": "X-qxOHx4cDTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVWKFLrzTrm0"
      },
      "outputs": [],
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Input(shape = (MAX_NUM_WORDS,1)))\n",
        "cnn_model.add(Dense(units = 128, activation = 'relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6QANlSKbrHP"
      },
      "source": [
        "Compile and fit the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\n",
        "cnn_model.add(MaxPooling1D(pool_size = 5))\n",
        "cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\n",
        "cnn_model.add(MaxPooling1D(pool_size = 5))\n",
        "cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\n",
        "cnn_model.add(GlobalMaxPool1D())\n",
        "cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
        "cnn_model.add(Dense(units = len(labels), activation = 'softmax'))\n",
        "\n",
        "print(cnn_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X55RUkX7r7hb",
        "outputId": "1b8efe00-efde-45ff-ed74-ef6c4c453b99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 100)         200000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 128)         64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 1032      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 445,768\n",
            "Trainable params: 245,768\n",
            "Non-trainable params: 200,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-fold cross validation"
      ],
      "metadata": {
        "id": "HgV0NIhxeHCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "FOLDS_NUM = 5\n",
        "\n",
        "train_labels_numpy = train_labels.to_numpy()\n",
        "acc_history = []\n",
        "loss_history = []\n",
        "kfold = KFold(n_splits=FOLDS_NUM, shuffle=True)\n",
        "\n",
        "for train, test in kfold.split(train_sequences, train_labels_numpy):\n",
        "  #Configures the model for training\n",
        "  cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "  es = EarlyStopping(monitor = \"accuracy\", min_delta = 0.01, patience = 10, verbose = 1, mode = 'auto')\n",
        "  mc = ModelCheckpoint(monitor = \"accuracy\", filepath = \"./bestmodel.h5\", verbose = 1, save_best_only = True, mode = 'auto')\n",
        "  cd = [es, mc]\n",
        "  #Trains the model for a fixed number of epochs (iterations on a dataset)\n",
        "  #history = cnn_model.fit(train_sequences[train], train_labels_numpy[train], batch_size = 128, epochs = 100, validation_data = (train_sequences[test], train_labels_numpy[test]), callbacks = [cd])\n",
        "  history = cnn_model.fit(train_sequences[train], train_labels_numpy[train], batch_size = 128, epochs = 100, callbacks = [cd])\n",
        "  #Save results\n",
        "  scores = cnn_model.evaluate(train_sequences[test], train_labels_numpy[test])\n",
        "  loss_history.append(scores[0])\n",
        "  acc_history.append(scores[1])\n",
        "\n",
        "acc_average = sum(acc_history) / len(acc_history)\n",
        "acc_stddev = ( sum([((x - acc_average) ** 2) for x in acc_history]) / len(acc_history) ) ** 0.5\n",
        "loss_average = sum(loss_history) / len(loss_history)\n",
        "loss_stddev = ( sum([((x - loss_average) ** 2) for x in loss_history]) / len(loss_history) ) ** 0.5\n",
        "print(f\"Accuracy: {acc_average} +- {acc_stddev}\")\n",
        "print(f\"Loss: {loss_average} +- {loss_stddev}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kft6Qf_yeGe5",
        "outputId": "e0635565-3120-4bc1-dc0a-4501bb713b06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.2853\n",
            "Epoch 1: accuracy improved from -inf to 0.28532, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 19s 26ms/step - loss: 0.3463 - accuracy: 0.2853\n",
            "Epoch 2/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.4869\n",
            "Epoch 2: accuracy improved from 0.28532 to 0.48685, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.2710 - accuracy: 0.4868\n",
            "Epoch 3/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.5687\n",
            "Epoch 3: accuracy improved from 0.48685 to 0.56872, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.2370 - accuracy: 0.5687\n",
            "Epoch 4/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.2103 - accuracy: 0.6254\n",
            "Epoch 4: accuracy improved from 0.56872 to 0.62565, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.2102 - accuracy: 0.6256\n",
            "Epoch 5/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.1895 - accuracy: 0.6698\n",
            "Epoch 5: accuracy improved from 0.62565 to 0.66971, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.1894 - accuracy: 0.6697\n",
            "Epoch 6/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.7024\n",
            "Epoch 6: accuracy improved from 0.66971 to 0.70233, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.1710 - accuracy: 0.7023\n",
            "Epoch 7/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.7373\n",
            "Epoch 7: accuracy improved from 0.70233 to 0.73720, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.1535 - accuracy: 0.7372\n",
            "Epoch 8/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.7638\n",
            "Epoch 8: accuracy improved from 0.73720 to 0.76382, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 7s 29ms/step - loss: 0.1384 - accuracy: 0.7638\n",
            "Epoch 9/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.7885\n",
            "Epoch 9: accuracy improved from 0.76382 to 0.78813, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 7s 30ms/step - loss: 0.1252 - accuracy: 0.7881\n",
            "Epoch 10/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.8070\n",
            "Epoch 10: accuracy improved from 0.78813 to 0.80700, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 7s 30ms/step - loss: 0.1145 - accuracy: 0.8070\n",
            "Epoch 11/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.8215\n",
            "Epoch 11: accuracy improved from 0.80700 to 0.82138, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.1050 - accuracy: 0.8214\n",
            "Epoch 12/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.8322\n",
            "Epoch 12: accuracy improved from 0.82138 to 0.83222, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0982 - accuracy: 0.8322\n",
            "Epoch 13/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.8418\n",
            "Epoch 13: accuracy improved from 0.83222 to 0.84180, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0927 - accuracy: 0.8418\n",
            "Epoch 14/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.8560\n",
            "Epoch 14: accuracy improved from 0.84180 to 0.85604, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0839 - accuracy: 0.8560\n",
            "Epoch 15/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.8559\n",
            "Epoch 15: accuracy did not improve from 0.85604\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0841 - accuracy: 0.8553\n",
            "Epoch 16/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.8613\n",
            "Epoch 16: accuracy improved from 0.85604 to 0.86130, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0796 - accuracy: 0.8613\n",
            "Epoch 17/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.8653\n",
            "Epoch 17: accuracy improved from 0.86130 to 0.86523, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 26ms/step - loss: 0.0781 - accuracy: 0.8652\n",
            "Epoch 18/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.8785\n",
            "Epoch 18: accuracy improved from 0.86523 to 0.87849, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0697 - accuracy: 0.8785\n",
            "Epoch 19/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.8824\n",
            "Epoch 19: accuracy improved from 0.87849 to 0.88224, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0676 - accuracy: 0.8822\n",
            "Epoch 20/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.8810\n",
            "Epoch 20: accuracy did not improve from 0.88224\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0688 - accuracy: 0.8808\n",
            "Epoch 21/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.8852\n",
            "Epoch 21: accuracy improved from 0.88224 to 0.88491, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0659 - accuracy: 0.8849\n",
            "Epoch 22/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.8814\n",
            "Epoch 22: accuracy did not improve from 0.88491\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0666 - accuracy: 0.8814\n",
            "Epoch 23/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.8849\n",
            "Epoch 23: accuracy did not improve from 0.88491\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0645 - accuracy: 0.8848\n",
            "Epoch 24/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.8927\n",
            "Epoch 24: accuracy improved from 0.88491 to 0.89252, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0617 - accuracy: 0.8925\n",
            "Epoch 25/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.8925\n",
            "Epoch 25: accuracy did not improve from 0.89252\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0619 - accuracy: 0.8924\n",
            "Epoch 26/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.8974\n",
            "Epoch 26: accuracy improved from 0.89252 to 0.89740, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0584 - accuracy: 0.8974\n",
            "Epoch 27/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.8975\n",
            "Epoch 27: accuracy did not improve from 0.89740\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0587 - accuracy: 0.8972\n",
            "Epoch 28/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.8965\n",
            "Epoch 28: accuracy did not improve from 0.89740\n",
            "223/223 [==============================] - 6s 27ms/step - loss: 0.0584 - accuracy: 0.8968\n",
            "Epoch 29/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0536 - accuracy: 0.9043\n",
            "Epoch 29: accuracy improved from 0.89740 to 0.90413, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0537 - accuracy: 0.9041\n",
            "Epoch 30/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9052\n",
            "Epoch 30: accuracy improved from 0.90413 to 0.90522, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0531 - accuracy: 0.9052\n",
            "Epoch 31/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9041\n",
            "Epoch 31: accuracy did not improve from 0.90522\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0539 - accuracy: 0.9041\n",
            "Epoch 32/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9088\n",
            "Epoch 32: accuracy improved from 0.90522 to 0.90876, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0501 - accuracy: 0.9088\n",
            "Epoch 33/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9055\n",
            "Epoch 33: accuracy did not improve from 0.90876\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0539 - accuracy: 0.9055\n",
            "Epoch 34/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9059\n",
            "Epoch 34: accuracy did not improve from 0.90876\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0530 - accuracy: 0.9062\n",
            "Epoch 35/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 0.9096\n",
            "Epoch 35: accuracy improved from 0.90876 to 0.90943, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 26ms/step - loss: 0.0504 - accuracy: 0.9094\n",
            "Epoch 36/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9129\n",
            "Epoch 36: accuracy improved from 0.90943 to 0.91294, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0494 - accuracy: 0.9129\n",
            "Epoch 37/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0479 - accuracy: 0.9141\n",
            "Epoch 37: accuracy improved from 0.91294 to 0.91416, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 26ms/step - loss: 0.0479 - accuracy: 0.9142\n",
            "Epoch 38/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0446 - accuracy: 0.9193\n",
            "Epoch 38: accuracy improved from 0.91416 to 0.91943, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0446 - accuracy: 0.9194\n",
            "Epoch 39/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9156\n",
            "Epoch 39: accuracy did not improve from 0.91943\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0482 - accuracy: 0.9155\n",
            "Epoch 40/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0491 - accuracy: 0.9134\n",
            "Epoch 40: accuracy did not improve from 0.91943\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0492 - accuracy: 0.9134\n",
            "Epoch 41/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9174\n",
            "Epoch 41: accuracy did not improve from 0.91943\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0459 - accuracy: 0.9174\n",
            "Epoch 42/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9227\n",
            "Epoch 42: accuracy improved from 0.91943 to 0.92269, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0424 - accuracy: 0.9227\n",
            "Epoch 43/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 0.9157\n",
            "Epoch 43: accuracy did not improve from 0.92269\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0470 - accuracy: 0.9157\n",
            "Epoch 44/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9182\n",
            "Epoch 44: accuracy did not improve from 0.92269\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0457 - accuracy: 0.9182\n",
            "Epoch 45/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9171\n",
            "Epoch 45: accuracy did not improve from 0.92269\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0461 - accuracy: 0.9171\n",
            "Epoch 46/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9194\n",
            "Epoch 46: accuracy did not improve from 0.92269\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0441 - accuracy: 0.9194\n",
            "Epoch 47/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9224\n",
            "Epoch 47: accuracy did not improve from 0.92269\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0429 - accuracy: 0.9224\n",
            "Epoch 47: early stopping\n",
            "223/223 [==============================] - 1s 5ms/step - loss: 0.7446 - accuracy: 0.5375\n",
            "Epoch 1/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.8136\n",
            "Epoch 1: accuracy improved from -inf to 0.81357, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 8s 28ms/step - loss: 0.1433 - accuracy: 0.8136\n",
            "Epoch 2/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.8457\n",
            "Epoch 2: accuracy improved from 0.81357 to 0.84559, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.1032 - accuracy: 0.8456\n",
            "Epoch 3/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.8632\n",
            "Epoch 3: accuracy improved from 0.84559 to 0.86334, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0878 - accuracy: 0.8633\n",
            "Epoch 4/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.8765\n",
            "Epoch 4: accuracy improved from 0.86334 to 0.87646, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0769 - accuracy: 0.8765\n",
            "Epoch 5/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.8852\n",
            "Epoch 5: accuracy improved from 0.87646 to 0.88530, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0710 - accuracy: 0.8853\n",
            "Epoch 6/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.8896\n",
            "Epoch 6: accuracy improved from 0.88530 to 0.88954, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0672 - accuracy: 0.8895\n",
            "Epoch 7/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.8919\n",
            "Epoch 7: accuracy improved from 0.88954 to 0.89175, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0651 - accuracy: 0.8918\n",
            "Epoch 8/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.8946\n",
            "Epoch 8: accuracy improved from 0.89175 to 0.89470, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0620 - accuracy: 0.8947\n",
            "Epoch 9/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9000\n",
            "Epoch 9: accuracy improved from 0.89470 to 0.90010, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0586 - accuracy: 0.9001\n",
            "Epoch 10/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0544 - accuracy: 0.9066\n",
            "Epoch 10: accuracy improved from 0.90010 to 0.90649, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0545 - accuracy: 0.9065\n",
            "Epoch 11/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9063\n",
            "Epoch 11: accuracy did not improve from 0.90649\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0545 - accuracy: 0.9063\n",
            "Epoch 12/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9076\n",
            "Epoch 12: accuracy improved from 0.90649 to 0.90764, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0535 - accuracy: 0.9076\n",
            "Epoch 13/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9133\n",
            "Epoch 13: accuracy improved from 0.90764 to 0.91322, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0495 - accuracy: 0.9132\n",
            "Epoch 14/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9154\n",
            "Epoch 14: accuracy improved from 0.91322 to 0.91543, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0484 - accuracy: 0.9154\n",
            "Epoch 15/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9130\n",
            "Epoch 15: accuracy did not improve from 0.91543\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0500 - accuracy: 0.9130\n",
            "Epoch 16/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9166\n",
            "Epoch 16: accuracy improved from 0.91543 to 0.91655, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0469 - accuracy: 0.9166\n",
            "Epoch 17/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0479 - accuracy: 0.9163\n",
            "Epoch 17: accuracy did not improve from 0.91655\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0479 - accuracy: 0.9161\n",
            "Epoch 18/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9165\n",
            "Epoch 18: accuracy improved from 0.91655 to 0.91666, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0471 - accuracy: 0.9167\n",
            "Epoch 19/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9201\n",
            "Epoch 19: accuracy improved from 0.91666 to 0.92017, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0451 - accuracy: 0.9202\n",
            "Epoch 20/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9227\n",
            "Epoch 20: accuracy improved from 0.92017 to 0.92269, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0432 - accuracy: 0.9227\n",
            "Epoch 21/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9260\n",
            "Epoch 21: accuracy improved from 0.92269 to 0.92599, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0406 - accuracy: 0.9260\n",
            "Epoch 22/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9216\n",
            "Epoch 22: accuracy did not improve from 0.92599\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0452 - accuracy: 0.9216\n",
            "Epoch 23/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9226\n",
            "Epoch 23: accuracy did not improve from 0.92599\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0437 - accuracy: 0.9225\n",
            "Epoch 24/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9215\n",
            "Epoch 24: accuracy did not improve from 0.92599\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0447 - accuracy: 0.9216\n",
            "Epoch 25/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9288\n",
            "Epoch 25: accuracy improved from 0.92599 to 0.92890, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0394 - accuracy: 0.9289\n",
            "Epoch 26/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9282\n",
            "Epoch 26: accuracy did not improve from 0.92890\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0390 - accuracy: 0.9282\n",
            "Epoch 27/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9274\n",
            "Epoch 27: accuracy did not improve from 0.92890\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0403 - accuracy: 0.9275\n",
            "Epoch 28/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9264\n",
            "Epoch 28: accuracy did not improve from 0.92890\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0414 - accuracy: 0.9263\n",
            "Epoch 29/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9279\n",
            "Epoch 29: accuracy did not improve from 0.92890\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0407 - accuracy: 0.9279\n",
            "Epoch 30/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9324\n",
            "Epoch 30: accuracy improved from 0.92890 to 0.93237, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0387 - accuracy: 0.9324\n",
            "Epoch 31/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0372 - accuracy: 0.9316\n",
            "Epoch 31: accuracy did not improve from 0.93237\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0371 - accuracy: 0.9317\n",
            "Epoch 31: early stopping\n",
            "223/223 [==============================] - 1s 5ms/step - loss: 0.2352 - accuracy: 0.7484\n",
            "Epoch 1/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.8826\n",
            "Epoch 1: accuracy improved from -inf to 0.88260, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 8s 25ms/step - loss: 0.0767 - accuracy: 0.8826\n",
            "Epoch 2/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9108\n",
            "Epoch 2: accuracy improved from 0.88260 to 0.91101, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0514 - accuracy: 0.9110\n",
            "Epoch 3/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9178\n",
            "Epoch 3: accuracy improved from 0.91101 to 0.91803, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0457 - accuracy: 0.9180\n",
            "Epoch 4/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9208\n",
            "Epoch 4: accuracy improved from 0.91803 to 0.92080, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0431 - accuracy: 0.9208\n",
            "Epoch 5/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0426 - accuracy: 0.9239\n",
            "Epoch 5: accuracy improved from 0.92080 to 0.92385, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 26ms/step - loss: 0.0427 - accuracy: 0.9238\n",
            "Epoch 6/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9247\n",
            "Epoch 6: accuracy improved from 0.92385 to 0.92466, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0406 - accuracy: 0.9247\n",
            "Epoch 7/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0455 - accuracy: 0.9190\n",
            "Epoch 7: accuracy did not improve from 0.92466\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0455 - accuracy: 0.9189\n",
            "Epoch 8/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9261\n",
            "Epoch 8: accuracy improved from 0.92466 to 0.92599, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0416 - accuracy: 0.9260\n",
            "Epoch 9/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0417 - accuracy: 0.9239\n",
            "Epoch 9: accuracy did not improve from 0.92599\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0416 - accuracy: 0.9241\n",
            "Epoch 10/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 0.9306\n",
            "Epoch 10: accuracy improved from 0.92599 to 0.93076, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0377 - accuracy: 0.9308\n",
            "Epoch 11/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9321\n",
            "Epoch 11: accuracy improved from 0.93076 to 0.93206, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0358 - accuracy: 0.9321\n",
            "Epoch 12/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9289\n",
            "Epoch 12: accuracy did not improve from 0.93206\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0400 - accuracy: 0.9289\n",
            "Epoch 13/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9274\n",
            "Epoch 13: accuracy did not improve from 0.93206\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0404 - accuracy: 0.9275\n",
            "Epoch 14/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9317\n",
            "Epoch 14: accuracy did not improve from 0.93206\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0362 - accuracy: 0.9316\n",
            "Epoch 15/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9324\n",
            "Epoch 15: accuracy improved from 0.93206 to 0.93248, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0358 - accuracy: 0.9325\n",
            "Epoch 15: early stopping\n",
            "223/223 [==============================] - 1s 5ms/step - loss: 0.0833 - accuracy: 0.8772\n",
            "Epoch 1/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9170\n",
            "Epoch 1: accuracy improved from -inf to 0.91690, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 8s 26ms/step - loss: 0.0489 - accuracy: 0.9169\n",
            "Epoch 2/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9309\n",
            "Epoch 2: accuracy improved from 0.91690 to 0.93086, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0386 - accuracy: 0.9309\n",
            "Epoch 3/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9348\n",
            "Epoch 3: accuracy improved from 0.93086 to 0.93483, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0349 - accuracy: 0.9348\n",
            "Epoch 4/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9367\n",
            "Epoch 4: accuracy improved from 0.93483 to 0.93669, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0344 - accuracy: 0.9367\n",
            "Epoch 5/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9313\n",
            "Epoch 5: accuracy did not improve from 0.93669\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0376 - accuracy: 0.9311\n",
            "Epoch 6/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9263\n",
            "Epoch 6: accuracy did not improve from 0.93669\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0409 - accuracy: 0.9263\n",
            "Epoch 7/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9312\n",
            "Epoch 7: accuracy did not improve from 0.93669\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0381 - accuracy: 0.9311\n",
            "Epoch 8/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9328\n",
            "Epoch 8: accuracy did not improve from 0.93669\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0382 - accuracy: 0.9329\n",
            "Epoch 9/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9380\n",
            "Epoch 9: accuracy improved from 0.93669 to 0.93798, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0344 - accuracy: 0.9380\n",
            "Epoch 10/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9380\n",
            "Epoch 10: accuracy did not improve from 0.93798\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0329 - accuracy: 0.9379\n",
            "Epoch 11/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9400\n",
            "Epoch 11: accuracy improved from 0.93798 to 0.93991, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0315 - accuracy: 0.9399\n",
            "Epoch 12/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0368 - accuracy: 0.9335\n",
            "Epoch 12: accuracy did not improve from 0.93991\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0368 - accuracy: 0.9335\n",
            "Epoch 12: early stopping\n",
            "223/223 [==============================] - 1s 4ms/step - loss: 0.0684 - accuracy: 0.8931\n",
            "Epoch 1/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9241\n",
            "Epoch 1: accuracy improved from -inf to 0.92416, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 7s 25ms/step - loss: 0.0440 - accuracy: 0.9242\n",
            "Epoch 2/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9347\n",
            "Epoch 2: accuracy improved from 0.92416 to 0.93462, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0356 - accuracy: 0.9346\n",
            "Epoch 3/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9358\n",
            "Epoch 3: accuracy improved from 0.93462 to 0.93584, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0351 - accuracy: 0.9358\n",
            "Epoch 4/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9349\n",
            "Epoch 4: accuracy did not improve from 0.93584\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0356 - accuracy: 0.9350\n",
            "Epoch 5/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9357\n",
            "Epoch 5: accuracy did not improve from 0.93584\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0352 - accuracy: 0.9357\n",
            "Epoch 6/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9364\n",
            "Epoch 6: accuracy improved from 0.93584 to 0.93648, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0348 - accuracy: 0.9365\n",
            "Epoch 7/100\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9406\n",
            "Epoch 7: accuracy improved from 0.93648 to 0.94062, saving model to ./bestmodel.h5\n",
            "223/223 [==============================] - 6s 25ms/step - loss: 0.0313 - accuracy: 0.9406\n",
            "Epoch 8/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9364\n",
            "Epoch 8: accuracy did not improve from 0.94062\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0350 - accuracy: 0.9363\n",
            "Epoch 9/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9351\n",
            "Epoch 9: accuracy did not improve from 0.94062\n",
            "223/223 [==============================] - 5s 25ms/step - loss: 0.0353 - accuracy: 0.9352\n",
            "Epoch 10/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9369\n",
            "Epoch 10: accuracy did not improve from 0.94062\n",
            "223/223 [==============================] - 5s 23ms/step - loss: 0.0351 - accuracy: 0.9369\n",
            "Epoch 11/100\n",
            "222/223 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9374\n",
            "Epoch 11: accuracy did not improve from 0.94062\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0332 - accuracy: 0.9374\n",
            "Epoch 12/100\n",
            "221/223 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9385\n",
            "Epoch 12: accuracy did not improve from 0.94062\n",
            "223/223 [==============================] - 5s 24ms/step - loss: 0.0327 - accuracy: 0.9385\n",
            "Epoch 12: early stopping\n",
            "223/223 [==============================] - 1s 6ms/step - loss: 0.0627 - accuracy: 0.8994\n",
            "Accuracy: 0.7911172151565552 +- 0.13834907381682127\n",
            "Loss: 0.2388216182589531 +- 0.2607912179693657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m_0Hs_1YN3H"
      },
      "source": [
        "Final validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-0QSRNwYP7w",
        "outputId": "158fcf64-f9c5-4788-8b8f-e7e9d0a1fd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 1s 12ms/step - loss: 0.8724 - accuracy: 0.5302\n",
            "test loss, test acc: [0.8723818063735962, 0.5302304625511169]\n"
          ]
        }
      ],
      "source": [
        "x_test = test_sequences\n",
        "y_test = test_labels\n",
        "print(\"test loss, test acc:\", cnn_model.evaluate(x_test, y_test, batch_size = 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final validation (but better, faster, stronger)"
      ],
      "metadata": {
        "id": "TXVgwqRUiphn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "x_test = test_sequences\n",
        "y_test = test_labels\n",
        "\n",
        "y_pred = cnn_model.predict(x_test, batch_size=128, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=-1)\n",
        "y_pred_array = []\n",
        "for i in y_pred_bool:\n",
        "  y_pred_array.append(np.zeros(len(labels)))\n",
        "  y_pred_array[-1][i] = 1\n",
        "\n",
        "print(classification_report(y_test, y_pred_array, target_names = labels))"
      ],
      "metadata": {
        "id": "Q1u44-JBimSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b3bf68-139f-44b8-a755-692302c5cc5a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 1s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.55      0.50      0.52      1486\n",
            "     disgust       0.61      0.55      0.58       746\n",
            "       anger       0.57      0.56      0.57      1410\n",
            "         joy       0.58      0.62      0.60      1483\n",
            "    surprise       0.43      0.51      0.47       892\n",
            "       trust       0.39      0.35      0.37       745\n",
            "     sadness       0.59      0.62      0.60      1475\n",
            "anticipation       0.39      0.36      0.37       744\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      8981\n",
            "   macro avg       0.51      0.51      0.51      8981\n",
            "weighted avg       0.53      0.53      0.53      8981\n",
            " samples avg       0.53      0.53      0.53      8981\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSt_NvilbwUc"
      },
      "source": [
        "Predicting and Submitting for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SislXB2qbyHY",
        "outputId": "413fb6f4-1af6-46ad-d777-d356a682125e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 117ms/step\n",
            "joy: beauty channel even though might well versed content conceptwise still enjoy much watching video delighted see choose present concept graphical effort show clearly coordination speech sometimes incredibly revelaing way teaching truly kinda sad often teacher mistake job giving concept student testing something along line work inspirational instructive enjoyable happy know big prop one collaborating make video confident saying like friend many u even though nt know person\n",
            "fear: master statistic still find type video invaluable refresh intuition\n",
            "joy: never enjoyed probability course much visualization really amazing\n",
            "anticipation: sooo going write chrome extension math u shopping amazon\n",
            "sadness: cold become nt want lose done caught gray\n",
            "sadness: even deepest pain even weakest hour darkest night lovely\n",
            "disgust: used adventurer like took arrow knee\n",
            "sadness: someone love\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [ \n",
        "    \"The beauty of this channel is that, even though I might be well versed with all of its content concept-wise, I still enjoy very much watching these videos. I'm just delighted to see how you choose to present concepts, the graphical effort to show them clearly, in coordination with speech. Sometimes in incredibly revelaing ways. This is what teaching is truly about, and it is kinda sad how often teachers themselves mistake their job as 'giving concepts to students' (...and testing them) (or something along those lines). Your work is very inspirational, instructive, enjoyable. I'm so happy that I know you - and big props to each one collaborating to make these videos. I'm confident in saying that you're like friends for many of us, even though we don't know you in person!\", \n",
        "    \"I did my masters in statistics an I still find these types of videos invaluable to refresh my own intuition.\" , \n",
        "    \"I have never enjoyed a probability course this much. these visualizations are really amazing\", \n",
        "    \"Sooo.... who is going to write a Chrome Extension to do the math for us when shopping on Amazon?\",\n",
        "    \"How cold have I become? I didn't want to lose you by what I'd done caught in the gray\",\n",
        "    \"Even in your deepest pain, even in your weakest hour, in your darkest night, you are lovely\",\n",
        "    \"I used to be an adventurer like you, but then I took an arrow in the knee\",\n",
        "    \"Can someone love me?\"\n",
        "    ]\n",
        "if preprocess:\n",
        "  test_sentences = [ p.Preprocess(i) for i in test_sentences ]\n",
        "o_test = np.array( [i for i in test_sentences] )\n",
        "o_test_vec = vectorizer.convert(o_test)\n",
        "o_test_results = np.argmax(cnn_model.predict(o_test_vec), axis=-1)\n",
        "\n",
        "for i in range(0, len(o_test)):\n",
        "  text = o_test[i]\n",
        "  emotion = list(train_labels.columns)[o_test_results[i]]\n",
        "  print(f\"{emotion}: {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REFERENCE - CLASSICAL BAG-OF-WORDS APPROACH"
      ],
      "metadata": {
        "id": "StNBqfr4cJ-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "all_words = []\n",
        "for text in train_texts:\n",
        "  all_words.extend( text.lower().split() )\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "word_features = list(all_words)[:MAX_NUM_WORDS]\n",
        "\n",
        "def text_features(text): \n",
        "    text_words = set(text.split()) \n",
        "    features = {}\n",
        "    for word in text_words:\n",
        "        features['contains({})'.format(word)] = (word in text_words)\n",
        "    return features\n",
        "\n",
        "train_set = []\n",
        "for i in range(len(train_texts)):\n",
        "  label = 0\n",
        "  for j in range( len(train_labels.columns) ):\n",
        "    if train_labels.iloc[i,j] == 1:\n",
        "      label = j\n",
        "  train_set.append( (train_texts[i], list(train_labels.columns)[label]) )\n",
        "print(train_set[0:10])\n",
        "\n",
        "test_set = []\n",
        "for i in range(len(test_texts)):\n",
        "  label = 0\n",
        "  for j in range( len(test_labels.columns) ):\n",
        "    if test_labels.iloc[i,j] == 1:\n",
        "      label = j\n",
        "  test_set.append( (test_texts[i], list(test_labels.columns)[label]) )\n",
        "print(test_set[0:10])\n",
        "\n",
        "train_set = [ ( text_features(text), emotion ) for (text, emotion) in train_set ]\n",
        "test_set = [ ( text_features(text), emotion ) for (text, emotion) in test_set ]\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urVEiHkOcM8D",
        "outputId": "040947f1-72f4-4ded-f5af-efb5c8854bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('USER USER USER anychance addressing communication sent yesterday still nt contact', 'fear'), ('life matter black life matter white couple handed water protestors asian man climbed stop light sign solidarity white woman cheered word encouragement their…', 'joy'), ('rip georgefloyd adding though guy wa police officer know law happened public knew witness camera knew would justifiable knew would go prison get', 'trust'), ('congrats nicole bb18 fix nose 😒💁🏻 bitter petty', 'anger'), ('alm alm alm alm alm alm alm black matter george floyd live mattered bad cop killed anyway gon na stop trial country way cop matter guilt get trial guilty trial first', 'trust'), ('USER — make feel vigorous fine kill said hestia display despair rather joyful —', 'sadness'), ('finally decided wa partially due fact feel post something important least interesting share', 'joy'), ('really study today chemistry playing madden way fun', 'anger'), ('george floyd story infuriated already hate cop furthers belief absolutely horrible innocent man dead', 'anger'), ('would interesting know doe matter care leg blown marching blacklivesmatter turned back country screw whoever', 'surprise')]\n",
            "[('USER nt fret another game 8 staytuned', 'sadness'), ('ive always giver taker feel selfish considering idea', 'anger'), ('everyone please good good others invite senseless violence stoop level protect fight cause fight justiceforgeorge blacklivesmatter protest', 'fear'), ('try complain show attack feel helpless like parent would', 'fear'), ('blm blacklivesmatter antifaterrorist happyjuneteenth pressed repeatedly say black life matter mike penny say life matter', 'joy'), ('john lewis towering figure civil right era dy 80 blacklivesmatter blm', 'fear'), ('stumbling quote quote urging really feel meant away hated day job dedicate effort matter', 'sadness'), ('george floyd wa arrested would make stats funny figure lie liar figure guess side equation fall', 'disgust'), ('beautiful bangor heybangor bangormeprotest bangorprotest blmprotest blacklivesmatter trumpprotest guilford trump impotus', 'joy'), ('black life matter sick tired stand blacklivesmatter enoughisenough georgefloydprotests justiceforgeorgefloyd', 'disgust')]\n",
            "0.6092862710165906\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}